(633, 512, 512, 1) (633, 512, 512, 1)
(71, 512, 512, 1) (71, 512, 512, 1)
Model: "model_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         (None, 512, 512, 1)       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 512, 512, 16)      160       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 256, 256, 16)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 256, 256, 32)      4640      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 128, 128, 32)      0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 128, 128, 64)      18496     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 64, 64, 64)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 64, 64, 128)       73856     
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 32, 32, 128)       0         
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 32, 32, 256)       295168    
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 16, 16, 256)       0         
_________________________________________________________________
dense_1 (Dense)              (None, 16, 16, 512)       131584    
_________________________________________________________________
up_sampling2d_1 (UpSampling2 (None, 32, 32, 512)       0         
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 32, 32, 256)       1179904   
_________________________________________________________________
up_sampling2d_2 (UpSampling2 (None, 64, 64, 256)       0         
_________________________________________________________________
conv2d_7 (Conv2D)            (None, 64, 64, 128)       295040    
_________________________________________________________________
up_sampling2d_3 (UpSampling2 (None, 128, 128, 128)     0         
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 128, 128, 64)      73792     
_________________________________________________________________
up_sampling2d_4 (UpSampling2 (None, 256, 256, 64)      0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 256, 256, 32)      18464     
_________________________________________________________________
up_sampling2d_5 (UpSampling2 (None, 512, 512, 32)      0         
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 512, 512, 1)       289       
=================================================================
Total params: 2,091,393
Trainable params: 2,091,393
Non-trainable params: 0
_________________________________________________________________
Train on 633 samples, validate on 71 samples
Epoch 1/100
 - 14s - loss: -3.9582e-01 - dice_coef: 0.3965 - val_loss: -4.0487e-01 - val_dice_coef: 0.4052
Epoch 2/100
 - 9s - loss: -4.3367e-01 - dice_coef: 0.4359 - val_loss: -5.9488e-01 - val_dice_coef: 0.5952
Epoch 3/100
 - 9s - loss: -3.5543e-01 - dice_coef: 0.3575 - val_loss: -5.6753e-01 - val_dice_coef: 0.5680
Epoch 4/100
 - 9s - loss: -5.9659e-01 - dice_coef: 0.5968 - val_loss: -6.2171e-01 - val_dice_coef: 0.6220
Epoch 5/100
 - 9s - loss: -6.0643e-01 - dice_coef: 0.6069 - val_loss: -6.0699e-01 - val_dice_coef: 0.6074
Epoch 6/100
 - 9s - loss: -6.0842e-01 - dice_coef: 0.6088 - val_loss: -6.2960e-01 - val_dice_coef: 0.6300
Epoch 7/100
 - 9s - loss: -6.0783e-01 - dice_coef: 0.6087 - val_loss: -6.2488e-01 - val_dice_coef: 0.6252
Epoch 8/100
 - 9s - loss: -6.1570e-01 - dice_coef: 0.6166 - val_loss: -6.3803e-01 - val_dice_coef: 0.6384
Epoch 9/100
 - 9s - loss: -7.3427e-01 - dice_coef: 0.7355 - val_loss: -8.0964e-01 - val_dice_coef: 0.8094
Epoch 10/100
 - 9s - loss: -8.6702e-01 - dice_coef: 0.8665 - val_loss: -9.0307e-01 - val_dice_coef: 0.9032
Epoch 11/100
 - 9s - loss: -9.0420e-01 - dice_coef: 0.9043 - val_loss: -9.1421e-01 - val_dice_coef: 0.9143
Epoch 12/100
 - 9s - loss: -9.1629e-01 - dice_coef: 0.9164 - val_loss: -9.2719e-01 - val_dice_coef: 0.9272
Epoch 13/100
 - 9s - loss: -9.2108e-01 - dice_coef: 0.9206 - val_loss: -9.2340e-01 - val_dice_coef: 0.9235
Epoch 14/100
 - 9s - loss: -9.2766e-01 - dice_coef: 0.9278 - val_loss: -9.1684e-01 - val_dice_coef: 0.9168
Epoch 15/100
 - 9s - loss: -9.2701e-01 - dice_coef: 0.9268 - val_loss: -9.3339e-01 - val_dice_coef: 0.9334
Epoch 16/100
 - 9s - loss: -9.3240e-01 - dice_coef: 0.9325 - val_loss: -9.3754e-01 - val_dice_coef: 0.9375
Epoch 17/100
 - 9s - loss: -9.3493e-01 - dice_coef: 0.9348 - val_loss: -9.3047e-01 - val_dice_coef: 0.9304
Epoch 18/100
 - 9s - loss: -9.3634e-01 - dice_coef: 0.9356 - val_loss: -9.4084e-01 - val_dice_coef: 0.9408
Epoch 19/100
 - 9s - loss: -9.3903e-01 - dice_coef: 0.9391 - val_loss: -9.4268e-01 - val_dice_coef: 0.9426
Epoch 20/100
 - 9s - loss: -9.4171e-01 - dice_coef: 0.9418 - val_loss: -9.4171e-01 - val_dice_coef: 0.9416
Epoch 21/100
 - 9s - loss: -9.4232e-01 - dice_coef: 0.9423 - val_loss: -9.4147e-01 - val_dice_coef: 0.9415
Epoch 22/100
 - 9s - loss: -9.4506e-01 - dice_coef: 0.9445 - val_loss: -9.2751e-01 - val_dice_coef: 0.9275
Epoch 23/100
 - 9s - loss: -9.4369e-01 - dice_coef: 0.9438 - val_loss: -9.4654e-01 - val_dice_coef: 0.9465
Epoch 24/100
 - 9s - loss: -9.4762e-01 - dice_coef: 0.9477 - val_loss: -9.4761e-01 - val_dice_coef: 0.9476
Epoch 25/100
 - 9s - loss: -9.4818e-01 - dice_coef: 0.9478 - val_loss: -9.4438e-01 - val_dice_coef: 0.9443
Epoch 26/100
 - 9s - loss: -9.4609e-01 - dice_coef: 0.9461 - val_loss: -9.4760e-01 - val_dice_coef: 0.9475
Epoch 27/100
 - 9s - loss: -9.4990e-01 - dice_coef: 0.9493 - val_loss: -9.4943e-01 - val_dice_coef: 0.9494
Epoch 28/100
 - 9s - loss: -9.5098e-01 - dice_coef: 0.9510 - val_loss: -9.4836e-01 - val_dice_coef: 0.9483
Epoch 29/100
 - 9s - loss: -9.5147e-01 - dice_coef: 0.9516 - val_loss: -9.4848e-01 - val_dice_coef: 0.9484
Epoch 30/100
 - 9s - loss: -9.5173e-01 - dice_coef: 0.9517 - val_loss: -9.4338e-01 - val_dice_coef: 0.9434
Epoch 31/100
 - 9s - loss: -9.3617e-01 - dice_coef: 0.9352 - val_loss: -9.4330e-01 - val_dice_coef: 0.9432
Epoch 32/100
 - 9s - loss: -9.4135e-01 - dice_coef: 0.9414 - val_loss: -9.3311e-01 - val_dice_coef: 0.9331
Epoch 33/100
 - 9s - loss: -9.5141e-01 - dice_coef: 0.9516 - val_loss: -9.5107e-01 - val_dice_coef: 0.9510
Epoch 34/100
 - 9s - loss: -9.5346e-01 - dice_coef: 0.9536 - val_loss: -9.4537e-01 - val_dice_coef: 0.9453
Epoch 35/100
 - 9s - loss: -9.5279e-01 - dice_coef: 0.9528 - val_loss: -9.5053e-01 - val_dice_coef: 0.9505
Epoch 36/100
 - 9s - loss: -9.5316e-01 - dice_coef: 0.9534 - val_loss: -9.5175e-01 - val_dice_coef: 0.9517
Epoch 37/100
 - 9s - loss: -9.5566e-01 - dice_coef: 0.9559 - val_loss: -9.5074e-01 - val_dice_coef: 0.9507
Epoch 38/100
 - 9s - loss: -9.5527e-01 - dice_coef: 0.9553 - val_loss: -9.5177e-01 - val_dice_coef: 0.9517
Epoch 39/100
 - 9s - loss: -9.5642e-01 - dice_coef: 0.9566 - val_loss: -9.4992e-01 - val_dice_coef: 0.9499
Epoch 40/100
 - 9s - loss: -9.5571e-01 - dice_coef: 0.9559 - val_loss: -9.5333e-01 - val_dice_coef: 0.9533
Epoch 41/100
 - 9s - loss: -9.5761e-01 - dice_coef: 0.9577 - val_loss: -9.5338e-01 - val_dice_coef: 0.9533
Epoch 42/100
 - 9s - loss: -9.5694e-01 - dice_coef: 0.9571 - val_loss: -9.5129e-01 - val_dice_coef: 0.9512
Epoch 43/100
 - 9s - loss: -9.5747e-01 - dice_coef: 0.9576 - val_loss: -9.5362e-01 - val_dice_coef: 0.9536
Epoch 44/100
 - 9s - loss: -9.5832e-01 - dice_coef: 0.9584 - val_loss: -9.5325e-01 - val_dice_coef: 0.9532
Epoch 45/100
 - 9s - loss: -9.5826e-01 - dice_coef: 0.9579 - val_loss: -9.5359e-01 - val_dice_coef: 0.9535
Epoch 46/100
 - 9s - loss: -9.5895e-01 - dice_coef: 0.9589 - val_loss: -9.5308e-01 - val_dice_coef: 0.9530
Epoch 47/100
 - 9s - loss: -9.5936e-01 - dice_coef: 0.9595 - val_loss: -9.5436e-01 - val_dice_coef: 0.9543
Epoch 48/100
 - 9s - loss: -9.5936e-01 - dice_coef: 0.9594 - val_loss: -9.5230e-01 - val_dice_coef: 0.9522
Epoch 49/100
 - 9s - loss: -9.5975e-01 - dice_coef: 0.9599 - val_loss: -9.5361e-01 - val_dice_coef: 0.9535
Epoch 50/100
 - 9s - loss: -9.5990e-01 - dice_coef: 0.9594 - val_loss: -9.5486e-01 - val_dice_coef: 0.9548
Epoch 51/100
 - 9s - loss: -9.5841e-01 - dice_coef: 0.9585 - val_loss: -9.5309e-01 - val_dice_coef: 0.9530
Epoch 52/100
 - 9s - loss: -9.5998e-01 - dice_coef: 0.9599 - val_loss: -9.5495e-01 - val_dice_coef: 0.9549
Epoch 53/100
 - 9s - loss: -9.6064e-01 - dice_coef: 0.9606 - val_loss: -9.5466e-01 - val_dice_coef: 0.9546
Epoch 54/100
 - 9s - loss: -9.6151e-01 - dice_coef: 0.9608 - val_loss: -9.5423e-01 - val_dice_coef: 0.9542
Epoch 55/100
 - 9s - loss: -9.6202e-01 - dice_coef: 0.9621 - val_loss: -9.5377e-01 - val_dice_coef: 0.9537
Epoch 56/100
 - 9s - loss: -9.6222e-01 - dice_coef: 0.9624 - val_loss: -9.5501e-01 - val_dice_coef: 0.9549
Epoch 57/100
 - 9s - loss: -9.6152e-01 - dice_coef: 0.9609 - val_loss: -9.5017e-01 - val_dice_coef: 0.9501
Epoch 58/100
 - 9s - loss: -9.6183e-01 - dice_coef: 0.9620 - val_loss: -9.5521e-01 - val_dice_coef: 0.9551
Epoch 59/100
 - 9s - loss: -9.6236e-01 - dice_coef: 0.9625 - val_loss: -9.5525e-01 - val_dice_coef: 0.9552
Epoch 60/100
 - 9s - loss: -9.6307e-01 - dice_coef: 0.9632 - val_loss: -9.5508e-01 - val_dice_coef: 0.9550
Epoch 61/100
 - 9s - loss: -9.6303e-01 - dice_coef: 0.9630 - val_loss: -9.5488e-01 - val_dice_coef: 0.9548
Epoch 62/100
 - 9s - loss: -9.6384e-01 - dice_coef: 0.9637 - val_loss: -9.5302e-01 - val_dice_coef: 0.9529
Epoch 63/100
 - 9s - loss: -9.6380e-01 - dice_coef: 0.9638 - val_loss: -9.5522e-01 - val_dice_coef: 0.9551
Epoch 64/100
 - 9s - loss: -9.6439e-01 - dice_coef: 0.9643 - val_loss: -9.5588e-01 - val_dice_coef: 0.9558
Epoch 65/100
 - 9s - loss: -9.6483e-01 - dice_coef: 0.9649 - val_loss: -9.5380e-01 - val_dice_coef: 0.9537
Epoch 66/100
 - 9s - loss: -9.6581e-01 - dice_coef: 0.9657 - val_loss: -9.5382e-01 - val_dice_coef: 0.9537
Epoch 67/100
 - 9s - loss: -9.6373e-01 - dice_coef: 0.9638 - val_loss: -9.5290e-01 - val_dice_coef: 0.9528
Epoch 68/100
 - 9s - loss: -9.6495e-01 - dice_coef: 0.9647 - val_loss: -9.5564e-01 - val_dice_coef: 0.9556
Epoch 69/100
 - 9s - loss: -9.6588e-01 - dice_coef: 0.9660 - val_loss: -9.5595e-01 - val_dice_coef: 0.9559
Epoch 70/100
 - 9s - loss: -9.6572e-01 - dice_coef: 0.9654 - val_loss: -9.5409e-01 - val_dice_coef: 0.9540
Epoch 71/100
 - 9s - loss: -9.6563e-01 - dice_coef: 0.9650 - val_loss: -9.5117e-01 - val_dice_coef: 0.9511
Epoch 72/100
 - 9s - loss: -9.6599e-01 - dice_coef: 0.9661 - val_loss: -9.5503e-01 - val_dice_coef: 0.9549

Epoch 00072: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.
Epoch 73/100
 - 9s - loss: -9.6855e-01 - dice_coef: 0.9687 - val_loss: -9.5609e-01 - val_dice_coef: 0.9560
Epoch 74/100
 - 9s - loss: -9.6884e-01 - dice_coef: 0.9689 - val_loss: -9.5616e-01 - val_dice_coef: 0.9561
Epoch 75/100
 - 9s - loss: -9.6909e-01 - dice_coef: 0.9692 - val_loss: -9.5609e-01 - val_dice_coef: 0.9560
Epoch 76/100
 - 9s - loss: -9.6924e-01 - dice_coef: 0.9694 - val_loss: -9.5594e-01 - val_dice_coef: 0.9559
Epoch 77/100
 - 9s - loss: -9.6948e-01 - dice_coef: 0.9696 - val_loss: -9.5585e-01 - val_dice_coef: 0.9558
Epoch 78/100
 - 9s - loss: -9.6947e-01 - dice_coef: 0.9695 - val_loss: -9.5601e-01 - val_dice_coef: 0.9559
Epoch 79/100
 - 9s - loss: -9.6970e-01 - dice_coef: 0.9698 - val_loss: -9.5578e-01 - val_dice_coef: 0.9557
Epoch 80/100
 - 9s - loss: -9.6971e-01 - dice_coef: 0.9694 - val_loss: -9.5578e-01 - val_dice_coef: 0.9557
Epoch 81/100
 - 9s - loss: -9.7012e-01 - dice_coef: 0.9694 - val_loss: -9.5585e-01 - val_dice_coef: 0.9558

Epoch 00081: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.
Epoch 82/100
 - 9s - loss: -9.7049e-01 - dice_coef: 0.9706 - val_loss: -9.5594e-01 - val_dice_coef: 0.9559
Epoch 83/100
 - 9s - loss: -9.7065e-01 - dice_coef: 0.9708 - val_loss: -9.5595e-01 - val_dice_coef: 0.9559
Epoch 84/100
 - 9s - loss: -9.7061e-01 - dice_coef: 0.9706 - val_loss: -9.5585e-01 - val_dice_coef: 0.9558
Epoch 85/100
 - 9s - loss: -9.7061e-01 - dice_coef: 0.9706 - val_loss: -9.5588e-01 - val_dice_coef: 0.9558
Epoch 86/100
 - 9s - loss: -9.7068e-01 - dice_coef: 0.9708 - val_loss: -9.5583e-01 - val_dice_coef: 0.9557
Epoch 87/100
 - 9s - loss: -9.7073e-01 - dice_coef: 0.9709 - val_loss: -9.5587e-01 - val_dice_coef: 0.9558
Epoch 88/100
 - 9s - loss: -9.7087e-01 - dice_coef: 0.9709 - val_loss: -9.5588e-01 - val_dice_coef: 0.9558
Epoch 89/100
 - 9s - loss: -9.7082e-01 - dice_coef: 0.9708 - val_loss: -9.5592e-01 - val_dice_coef: 0.9558

Epoch 00089: ReduceLROnPlateau reducing learning rate to 1e-05.
Epoch 90/100
 - 9s - loss: -9.7093e-01 - dice_coef: 0.9702 - val_loss: -9.5587e-01 - val_dice_coef: 0.9558
Epoch 91/100
 - 9s - loss: -9.7075e-01 - dice_coef: 0.9708 - val_loss: -9.5591e-01 - val_dice_coef: 0.9558
Epoch 92/100
 - 9s - loss: -9.7097e-01 - dice_coef: 0.9711 - val_loss: -9.5577e-01 - val_dice_coef: 0.9557
Epoch 93/100
 - 9s - loss: -9.7100e-01 - dice_coef: 0.9711 - val_loss: -9.5587e-01 - val_dice_coef: 0.9558
Epoch 94/100
 - 9s - loss: -9.7104e-01 - dice_coef: 0.9700 - val_loss: -9.5592e-01 - val_dice_coef: 0.9558
Epoch 95/100
 - 9s - loss: -9.7102e-01 - dice_coef: 0.9712 - val_loss: -9.5570e-01 - val_dice_coef: 0.9556
Epoch 96/100
 - 9s - loss: -9.7106e-01 - dice_coef: 0.9711 - val_loss: -9.5584e-01 - val_dice_coef: 0.9558
Epoch 97/100
 - 9s - loss: -9.7106e-01 - dice_coef: 0.9709 - val_loss: -9.5578e-01 - val_dice_coef: 0.9557
Epoch 98/100
 - 9s - loss: -9.7119e-01 - dice_coef: 0.9712 - val_loss: -9.5580e-01 - val_dice_coef: 0.9557
Epoch 99/100
 - 9s - loss: -9.7112e-01 - dice_coef: 0.9704 - val_loss: -9.5577e-01 - val_dice_coef: 0.9557
Epoch 100/100
 - 9s - loss: -9.7121e-01 - dice_coef: 0.9707 - val_loss: -9.5577e-01 - val_dice_coef: 0.9557
